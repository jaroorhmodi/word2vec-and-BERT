{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/word2vec-and-BERT/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ursbIzV2v9p7"
      },
      "source": [
        "#BERT (Bidirectional Encoder Representations from Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hEcTXgYwL-N"
      },
      "source": [
        "In this notebook I will be replicating the model in the paper [**BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805.pdf).\n",
        "\n",
        "While I will be creating the model from (mostly) scratch in PyTorch, I will not go into too much detail about why Multi-Head Attention is designed the way it is and how exactly the original [Transformer](https://arxiv.org/abs/1706.03762) architecture works. I have made another (*albeit messy*) [notebook that covers that paper](https://github.com/jaroorhmodi/transformer-from-scratch).\n",
        "\n",
        "The model will be trained on the [**WikiText-2**](https://paperswithcode.com/dataset/wikitext-2) and [**Wikitext-103**](https://paperswithcode.com/dataset/wikitext-103) datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Koq_QwuW4MbN",
        "outputId": "9bb112e1-e168-48b5-9f9f-af3486fe9b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting portalocker\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, portalocker, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 portalocker-2.8.2 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker transformers tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SWkkb9uc4T5B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import spacy\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103 #our datasets for this project\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DATASET_small = \"WikiText2\"\n",
        "DATASET_large = \"WikiText103\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TOKENIZER=\"basic_english\"\n",
        "DATA_DIRECTORY = \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09DBB2wd24DJ"
      },
      "source": [
        "##Model Objective and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVv47JM5I7p"
      },
      "source": [
        "###Who (What) is BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCHzPRMbL6Pw"
      },
      "source": [
        "\n",
        "While BERT is a much more complex model and what it accomplishes isn't exactly akin to Word2Vec, the intuition behind both is similar. We pass in sentences and attempt to make a model learn how to represent text in a way that captures not only information about the tokens themselves but also something about their *meaning*.\n",
        "\n",
        "Word2Vec does this by training a model on words and their context in sentences and learning  about their relationships with one another by either trying to predict context from words (*Skip-Gram*) or words from context (*CBOW*). The embeddings it produces are static for each word.\n",
        "\n",
        "BERT trains a Transformer Encoder model on two specific objectives: *Masked Language Modeling* and *Next Sentence Prediction* to learn a wealth of information about tokens in their context and provide representations of them. Note that BERT is not simply learning static embeddings but rather representations that change based on context. Tokens in BERT are embedded using *WordPiece* embeddings.\n",
        "\n",
        "The goal of the BERT paper was to introduce a way to represent words with a pre-trained transformer and not to make a model for a specific predictive goal. To this end it is trained in an unsupervised manner with the aforementioned MLM and NSP objectives (will be explained ahead)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZh1_Ky2gPKa"
      },
      "source": [
        "###Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "MjWVBG8Ff9jY"
      },
      "outputs": [],
      "source": [
        "#We need to pull in the dataset and break it into sentence pairs for the NSP objective\n",
        "#and we need to mask random words and create objectives for the MLM objective.\n",
        "DATASET = DATASET_small\n",
        "dataset_class = WikiText2 if DATASET == DATASET_small else WikiText103\n",
        "data_train = dataset_class(DATA_DIRECTORY, split = \"train\")\n",
        "\n",
        "OPT_VERSION = ''\n",
        "\n",
        "TOKENS_LOCATION = os.path.join(DATA_DIRECTORY, \"datasets\", DATASET, DATASET.lower()[:8]+f\"-{DATASET[8:]}\")\n",
        "TOKENIZER_LOCATION = os.path.join(DATA_DIRECTORY, \"tokenizers\", )\n",
        "TOKENIZER_NAME = f\"bert-wordpiece-{DATASET}{OPT_VERSION}\" #just here to standardize naming for later\n",
        "os.makedirs(TOKENIZER_LOCATION, exist_ok = True)\n",
        "\n",
        "MAX_TOKENIZED_SENTENCE_LEN = 128 #maximum number of tokens in sentence\n",
        "\n",
        "gen = iter(data_train)\n",
        "\n",
        "sample = []\n",
        "for i in range(20):\n",
        "  sample.append(next(gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVIW8MfNiYX-",
        "outputId": "02358280-316b-4d24-b60e-5502efbfaf69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(sample[:5])\n",
        "print(len(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx3b0r8KkA8x"
      },
      "source": [
        "In the WikiText data, we see a lot of control characters like newlines, we want to make sure when we tokenize that we do not consider these. We also see that articles are delineated by a header given between single **=** signs and subheaders given by double, triple etc. equal signs.\n",
        "\n",
        "We don't want header paragraphs and we don't want empty whitespace lines, so we need to preprocess the data some before we actually split them out into sentence pairs for our NSP task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXKcxThinJxC",
        "outputId": "aed4d9ea-b3ae-4dec-fe77-0b9d928f2c6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['./tokenizers/bert-wordpiece-WikiText2-vocab.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text = True, #removes control chars like \\n\n",
        "    handle_chinese_chars = False, #not anticipating chinese chars\n",
        "    strip_accents = False, #keep accents in text\n",
        "    lowercase = True #ignore case\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files = os.path.join(TOKENS_LOCATION, \"wiki.train.tokens\"),\n",
        "    vocab_size = 30_000 if DATASET == DATASET_small else 90_000, #bigger vocab for bigger dataset\n",
        "    min_frequency = 10 if DATASET == DATASET_small else 50, #require higher freq for bigger dataset\n",
        "    limit_alphabet = 1000,\n",
        "    wordpieces_prefix = '##',\n",
        "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]']\n",
        ")\n",
        "\n",
        "tokenizer.save_model(\n",
        "    TOKENIZER_LOCATION,\n",
        "    TOKENIZER_NAME\n",
        ")\n",
        "\n",
        "#This is the tokenizer we will use\n",
        "tokenizer = BertTokenizer.from_pretrained(os.path.join(TOKENIZER_LOCATION, TOKENIZER_NAME)+'-vocab.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkaIC0CR0k8u"
      },
      "source": [
        "\n",
        "All of the input text for BERT training is to be of the form\n",
        "\n",
        "    \"[CLS] <SENTENCE1> [SEP] <SENTENCE2> [SEP]\"\n",
        "with `[PAD]` tokens added at the end as needed.\n",
        "\n",
        "**Notice below how `token_type_ids` denotes where the first sequence ends and the next begins.**\n",
        "\n",
        "Data preprocessing will require us to set up the two  training tasks of NSP and MLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrJwEZxG53xO",
        "outputId": "1f6cbb43-9957-4da2-957f-585e4fed92a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test=(' = Valkyria Chronicles III = \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n')\n",
            "__input_ids__\n",
            "len=1\n",
            "tensor([[    1,    33,  7754,  7833,  2907,    33,     2,  2957,  5195,   746,\n",
            "          7754,    23,    30,    32,   396,    34,  7833,    12,  2709,    30,\n",
            "           236,   234,     4,    16,  1244,    18,  7754,   394,   381,  9510,\n",
            "            23,    13,    16,  4627,  3155,   403,   428,  7754,  7833,  2907,\n",
            "          2364,  1826,    16,   445,    42, 10526,  1554,    36,    17,    36,\n",
            "          2202,  1380,   741,  1919,   450,  5160,   252,   399,  2639,    18,\n",
            "          5480,   424,   381,  4501, 10289,    18,  1146,   391,  1425,  1719,\n",
            "           391,  1826,    16,   444,   445,   381,  1361,   741,   391,   381,\n",
            "          7754,   885,    18,    32,   396,    34,   381,  1145,  8559,   394,\n",
            "         10526,   399,  1511,    36,    17,    36,   689,  5059,   428,   577,\n",
            "         12405,    16,   381,  1422,  3291,  6416,   403,   381,   573,   741,\n",
            "           399,  4397,   381,     6, 18266,     6,    16,    42, 11551,  1594,\n",
            "          3147,  5034,   381,  3659,   394,  3359,   566,   677,   381,   843,\n",
            "          9689,   251,   758,   637,   967,  2461,  1288,  2283,   399,   520,\n",
            "         14511,   384,   972,   381,  5322,  3147,     6,    32,   396,    34,\n",
            "         18511,     6,    18,     2]])\n",
            "__token_type_ids__\n",
            "len=1\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
            "__attention_mask__\n",
            "len=1\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "test = (sample[1], sample[3])\n",
        "\n",
        "# test = \"\\n\"\n",
        "print(f\"{test=}\")\n",
        "#automatically adds [CLS] before first sentence and [SEP] after each sentence\n",
        "#this is how Bert separates two sentences in the input\n",
        "tokenized = tokenizer.encode_plus(test[0], test[1], add_special_tokens = True, return_tensors = \"pt\")\n",
        "for key, val in tokenized.items():\n",
        "  print(f\"__{key}__\")\n",
        "  print(f\"len={len(val)}\")\n",
        "  print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsHMzNC76hQc"
      },
      "outputs": [],
      "source": [
        "def new_article(para):\n",
        "  #just checks if it is a new article\n",
        "  return para.strip().startswith(\"= \") and not para.strip().startswith(\"= =\")\n",
        "\n",
        "def new_heading(para):\n",
        "  #checks if it is a new heading\n",
        "  #slightly different from the new article\n",
        "  return para.strip().startswith(\"=\")\n",
        "\n",
        "def split_sentences(para):\n",
        "  sentences = para.split('. ')\n",
        "\n",
        "def article_sentence_pairs(article_sentences):\n",
        "  #See note below\n",
        "  sentence_pairs = []\n",
        "  for i, sentence in enumerate(article_sentences):\n",
        "    if i == len(article_sentences)-1:\n",
        "      break\n",
        "    else:\n",
        "      sentence_pairs.append((sentence, article_sentences[i+1]))\n",
        "  return sentence_pairs\n",
        "\n",
        "def collect_sentence_pairs(dataset_iter):\n",
        "  articles = []\n",
        "  current_article = []\n",
        "  for para in tqdm(dataset_iter):\n",
        "    if new_article(para):\n",
        "      #new article, stop collecting sentences\n",
        "      if len(current_article) > 0:\n",
        "        articles += article_sentence_pairs(current_article)\n",
        "      current_article = []\n",
        "      continue\n",
        "    if para.strip() == '' or new_heading(para):\n",
        "      #new heading or empty line, skip and continue collecting\n",
        "      continue\n",
        "    current_article += para.strip().split('. ')\n",
        "  return articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOAOtjrPesbQ"
      },
      "source": [
        "NOTE: there are a few ways we could have split this dataset. I chose to go with one that splits at `\". \"` for this approach but we could have tokenized the data first and split according to maximum sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URu40H5ReqGc"
      },
      "outputs": [],
      "source": [
        "#proper pipeline management, since we iterated once on the old data_train object we reset iteration\n",
        "gen = iter(dataset_class(DATA_DIRECTORY, split = \"train\"))\n",
        "sentence_pairs = collect_sentence_pairs(gen)\n",
        "len(sentence_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmojwWO6lddp"
      },
      "outputs": [],
      "source": [
        "sentence_pairs[900000:900010]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Persist sentence pairs to file\n",
        "os.makedirs(os.path.join(DATA_DIRECTORY, \"datasets\", \"pairs_data\"), exist_ok = True)\n",
        "\n",
        "\n",
        "SENTENCE_PAIRS_LOCATION = os.path.join(TOKENS_LOCATION, \"pairs\")\n",
        "os.makedirs(SENTENCE_PAIRS_LOCATION)\n",
        "\n",
        "\n",
        "#pickle 200k sentence pairs at a time into various pickle files\n",
        "def pickle_pairs(pairs, chunk_size = 200_000):\n",
        "  i = 0\n",
        "  while chunk_size*(i+1) < len(pairs):\n",
        "    with open(os.path.join(SENTENCE_PAIRS_LOCATION, f\"{i}.pkl\"), \"wb\") as f:\n",
        "      pickle.dump(pairs[chunk_size*i:chunk_size*(i+1)], f)\n",
        "    i+=1\n",
        "\n",
        "pickle_pairs(sentence_pairs)\n"
      ],
      "metadata": {
        "id": "W3HiEHtMj4ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see now that we are able to make pairs of sentences next to one another. Note that our particular method makes it so the first sentence from the immediately following paragraph is treated as a \"next sentence\" but the first sentence of the following article is not."
      ],
      "metadata": {
        "id": "Ra6V0_gbBfky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Two Training Objectives"
      ],
      "metadata": {
        "id": "P2bslE5_CDLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The following examples are straight from the paper and used to illustrate the NSP objective but can be used to explain the MLM objective as well.\n",
        "    \n",
        "    Input: [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
        "    Label: IsNext\n",
        "\n",
        "    Input: [CLS] the man went to [MASK] store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
        "    Label: NotNext"
      ],
      "metadata": {
        "id": "CfRzg63aDJRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Masked Language Model\n",
        "The **Masked Language Model** task masks out a given ratio of the tokens (and with some small probability substitutes with a random token) in the inputs and asks the model to predict the tokens that were masked out."
      ],
      "metadata": {
        "id": "iEACS1_SC14n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Next Sentence Prediction\n",
        "\n",
        "The **Next Sentence Prediction** task takes sentence pairs and creates a balanced classification task, where half of the time the following sentence is actually the next sentence and half of the time it is a random sentence. Then the model is trained to correctly predict whether or not the second sentence is the actual next sentence.\n",
        "\n",
        "We have already done half of the work for this task by creating positive next sentence pairs for each sentence in the text. All we have to do is replace the second sentence in each pair with a random sentence half of the time."
      ],
      "metadata": {
        "id": "mBlyctbQETyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will create a custom Dataset class for our purposes\n",
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, sentence_pairs, tokenizer, max_len=512):\n",
        "    self.sentence_pairs = sentence_pairs\n",
        "    self.num_pairs = len(sentence_pairs)\n",
        "    self.tokenizer = tokenizer #we may not need this\n",
        "    self.max_len = max_len #we may not need this\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_pairs\n",
        "\n",
        "  #The key component of a Dataset is the __getitem__ function\n",
        "  def __getitem__(self, idx):\n",
        "    sentence1, sentence2, isNextLabel = self.get_nsp_entry(idx)\n",
        "\n",
        "  def get_nsp_entry(self, idx):\n",
        "    #Implement NSP randomization here\n",
        "    sent1, sent2 = self.sentence_pairs[idx]\n",
        "    if random.random() >= 0.5:\n",
        "      #this is the case where we give positive nsp example\n",
        "      return sent1, sent2, 1\n",
        "    else:\n",
        "      return sent1, self.get_non_next_sentence(idx), 0\n",
        "\n",
        "  def get_non_next_sentence(self, idx):\n",
        "    random_idx = random.randrange(self.num_pairs)\n",
        "    while random_idx == idx:\n",
        "      \"\"\"\n",
        "      this is just here for the small chance that\n",
        "      our random index maps to the same one and gives\n",
        "      us a false pair where the actual next sentence\n",
        "      is mislabeled as NotNext\n",
        "      \"\"\"\n",
        "      random_idx = random.randrange(self.num_pairs)\n",
        "    return self.sentence_pairs[random_idx][1]\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "lvIguW-eBfVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Example using the Transformers Library for MLM\n",
        "\n",
        "We can use the `transformers.DataCollatorForLanguageModeling` class to handle the masking for us.\n",
        "\n",
        "We see [in the documentation](https://github.com/huggingface/transformers/blob/b71f20a7c9f3716d30f6738501559acf863e2c5c/src/transformers/data/data_collator.py#L751C1-L751C108) that exactly like in the paper, `DataCollatorForLanguageModeling` will mask tokens 80% of the time, replace with a random token 10% of the time, and leave as is 10% of the time. This is all conditional on the `mlm_probability` value passed into the collator which is `0.15` in the paper."
      ],
      "metadata": {
        "id": "EBZIiacqlQPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = True, mlm_probability = 0.15)"
      ],
      "metadata": {
        "id": "kk9NJ5exV-9S"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We recreate our tokenization example from earlier,\n",
        "#we just want to create a sentence pair for illustration\n",
        "feats = tokenizer.encode_plus(sample[1], sample[3], add_special_tokens = True, return_tensors = \"pt\")"
      ],
      "metadata": {
        "id": "W4t3AtDVcJ7U"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "masked = collator([feats])\n",
        "masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bq17uOXaunC",
        "outputId": "8cecc102-d43c-494e-f8da-ac1169740116"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[[    1,    33,  7754,  7833,  2907,    33,     2,  2957,  5195,   746,\n",
              "           7754,    23,    30,    32,   396,     3,  7833,    12, 15373,    30,\n",
              "            236,   234,     4,    16,  1244,    18,  7754,   394,   381,  9510,\n",
              "             23,    13,    16,  4627,  3155,   403,   428,  7754,  7833,  2907,\n",
              "           2364,     3,    16,   445,    42,     3,  1554,    36,    17,    36,\n",
              "           2202,  1380,   741,  1919,   450,  5160,   252,   399,  2639,    18,\n",
              "           5480,   424,   381,  4501, 10289,    18,  1146,     3,  1425,  1719,\n",
              "            391,  1826,    16,   444,     3,   381,  1361,   741,   391,     3,\n",
              "           7754,   885,    18,    32,   396,    34,     3,  1145,  8559,   394,\n",
              "          10526,   399,  1511,    36,    17,    36,   689,  5059,   428,   577,\n",
              "          12405,    16,   381,  1422,  3291,  6416,     3,     3,   573,   741,\n",
              "            399,  4397,   381,     3, 18266,     6,    16,    42, 11551,  1594,\n",
              "           3147,  5034,   381,     3,   394,  3359,     3,   677,   381,   843,\n",
              "           9689,   251,   758,     3,   967,  3496,  1288,  2283,   399,   520,\n",
              "          14511,   384,     3,   381,     3,  3147,     6,    32,   396,    34,\n",
              "          18511,     6,    18,     2]]]), 'token_type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]]), 'labels': tensor([[[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,    34,  -100,  -100,  2709,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  2907,\n",
              "           -100,  1826,  -100,  -100,  -100, 10526,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,   391,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,   445,  -100,  -100,  -100,  -100,   381,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,   381,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,   403,   381,  -100,  -100,\n",
              "           -100,  -100,  -100,     6,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  3659,  -100,  -100,   566,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,   637,  -100,  2461,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,   972,  -100,  5322,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(masked['input_ids'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "zcGK-zY3kLp0",
        "outputId": "2226f0ec-12b9-4163-ab6c-c85a73819710"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] = valkyria chronicles iii = [SEP] senjo no valkyria 3 : < unk [MASK] chronicles ( criticizing : 戦 場 [UNK], lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside [MASK], is a [MASK] role @ - @ playing video game developed by sega and media. vision for the playstation portable. released [MASK] january 2011 in japan, it [MASK] the third game in [MASK] valkyria series. < unk > [MASK] same fusion of tactical and real @ - @ time gameplay as its predecessors, the story runs parallel [MASK] [MASK] first game and follows the [MASK] nameless \", a penal military unit serving the [MASK] of gall [MASK] during the second europan war [MASK] performanced black operations and are pitted [MASK] the [MASK] unit \" < unk > raven \". [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(feats['input_ids'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "3CwE_vdzkuST",
        "outputId": "db92b711-e170-4e2f-c80a-d938b7b622f8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] = valkyria chronicles iii = [SEP] senjo no valkyria 3 : < unk > chronicles ( japanese : 戦 場 [UNK], lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside japan, is a tactical role @ - @ playing video game developed by sega and media. vision for the playstation portable. released in january 2011 in japan, it is the third game in the valkyria series. < unk > the same fusion of tactical and real @ - @ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" < unk > raven \". [SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rorGbQJC29w5"
      },
      "source": [
        "##Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD18JF5qvtId"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prBAUhze37Qv"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MakZRzAE6ESt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqpb8lvLMBkNJd2ElDbJX7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}