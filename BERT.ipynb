{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/word2vec-and-BERT/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ursbIzV2v9p7"
      },
      "source": [
        "#BERT (Bidirectional Encoder Representations from Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hEcTXgYwL-N"
      },
      "source": [
        "In this notebook I will be replicating the model in the paper [**BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805.pdf).\n",
        "\n",
        "While I will be creating the model from (mostly) scratch in PyTorch, I will not go into too much detail about why Multi-Head Attention is designed the way it is and how exactly the original [Transformer](https://arxiv.org/abs/1706.03762) architecture works. I have made another (*albeit messy*) [notebook that covers that paper](https://github.com/jaroorhmodi/transformer-from-scratch).\n",
        "\n",
        "The model will be trained on the [**WikiText-2**](https://paperswithcode.com/dataset/wikitext-2) and [**Wikitext-103**](https://paperswithcode.com/dataset/wikitext-103) datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Koq_QwuW4MbN"
      },
      "outputs": [],
      "source": [
        "!pip install portalocker transformers tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "SWkkb9uc4T5B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import copy\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103 #our datasets for this project\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
        "\n",
        "import math\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import random\n",
        "import spacy\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DATASET_small = \"WikiText2\"\n",
        "DATASET_large = \"WikiText103\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TOKENIZER=\"basic_english\"\n",
        "DATA_DIRECTORY = \".\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09DBB2wd24DJ"
      },
      "source": [
        "##Model Objective and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVv47JM5I7p"
      },
      "source": [
        "###Who (What) is BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCHzPRMbL6Pw"
      },
      "source": [
        "\n",
        "While BERT is a much more complex model and what it accomplishes isn't exactly akin to Word2Vec, the intuition behind both is similar. We pass in sentences and attempt to make a model learn how to represent text in a way that captures not only information about the tokens themselves but also something about their *meaning*.\n",
        "\n",
        "Word2Vec does this by training a model on words and their context in sentences and learning  about their relationships with one another by either trying to predict context from words (*Skip-Gram*) or words from context (*CBOW*). The embeddings it produces are static for each word.\n",
        "\n",
        "BERT trains a Transformer Encoder model on two specific objectives: *Masked Language Modeling* and *Next Sentence Prediction* to learn a wealth of information about tokens in their context and provide representations of them. Note that BERT is not simply learning static embeddings but rather representations that change based on context. Tokens in BERT are embedded using *WordPiece* embeddings.\n",
        "\n",
        "The goal of the BERT paper was to introduce a way to represent words with a pre-trained transformer and not to make a model for a specific predictive goal. To this end it is trained in an unsupervised manner with the aforementioned MLM and NSP objectives (will be explained ahead)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZh1_Ky2gPKa"
      },
      "source": [
        "###Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjWVBG8Ff9jY"
      },
      "outputs": [],
      "source": [
        "#We need to pull in the dataset and break it into sentence pairs for the NSP objective\n",
        "#and we need to mask random words and create objectives for the MLM objective.\n",
        "DATASET = DATASET_small\n",
        "dataset_class = WikiText2 if DATASET == DATASET_small else WikiText103\n",
        "data_train = dataset_class(DATA_DIRECTORY, split = \"train\")\n",
        "\n",
        "OPT_VERSION = ''\n",
        "\n",
        "TOKENS_LOCATION = os.path.join(DATA_DIRECTORY, \"datasets\", DATASET, DATASET.lower()[:8]+f\"-{DATASET[8:]}\")\n",
        "TOKENIZER_LOCATION = os.path.join(DATA_DIRECTORY, \"tokenizers\", )\n",
        "TOKENIZER_NAME = f\"bert-wordpiece-{DATASET}{OPT_VERSION}\" #just here to standardize naming for later\n",
        "os.makedirs(TOKENIZER_LOCATION, exist_ok = True)\n",
        "\n",
        "MAX_TOKENIZED_SENTENCE_LEN = 128 #maximum number of tokens in sentence\n",
        "\n",
        "gen = iter(data_train)\n",
        "\n",
        "sample = []\n",
        "for i in range(20):\n",
        "  sample.append(next(gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVIW8MfNiYX-",
        "outputId": "b50ed3a9-d198-4d2e-86a9-6853075b5cfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(sample[:5])\n",
        "print(len(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx3b0r8KkA8x"
      },
      "source": [
        "In the WikiText data, we see a lot of control characters like newlines, we want to make sure when we tokenize that we do not consider these. We also see that articles are delineated by a header given between single **=** signs and subheaders given by double, triple etc. equal signs.\n",
        "\n",
        "We don't want header paragraphs and we don't want empty whitespace lines, so we need to preprocess the data some before we actually split them out into sentence pairs for our NSP task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXKcxThinJxC",
        "outputId": "c0826931-b8c0-43cd-99e3-928c074f0ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1946: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
            "  warnings.warn(\n",
            "loading file ./tokenizers/bert-wordpiece-WikiText2-vocab.txt from cache at ./tokenizers/bert-wordpiece-WikiText2-vocab.txt\n",
            "Adding [UNK] to the vocabulary\n",
            "Adding [SEP] to the vocabulary\n",
            "Adding [PAD] to the vocabulary\n",
            "Adding [CLS] to the vocabulary\n",
            "Adding [MASK] to the vocabulary\n",
            "Assigning ['<unk>'] to the additional_special_tokens key of the tokenizer\n",
            "Adding <unk> to the vocabulary\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ],
      "source": [
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text = True, #removes control chars like \\n\n",
        "    handle_chinese_chars = False, #not anticipating chinese chars\n",
        "    strip_accents = False, #keep accents in text\n",
        "    lowercase = True #ignore case\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files = os.path.join(TOKENS_LOCATION, \"wiki.train.tokens\"),\n",
        "    vocab_size = 30_000 if DATASET == DATASET_small else 90_000, #bigger vocab for bigger dataset\n",
        "    min_frequency = 10 if DATASET == DATASET_small else 50, #require higher freq for bigger dataset\n",
        "    limit_alphabet = 1000,\n",
        "    wordpieces_prefix = '##',\n",
        "    special_tokens=['[PAD]', '[CLS]', '[SEP]', '[MASK]', '[UNK]'] #in wikitext, <unk> is used for unknown tokens\n",
        ")\n",
        "\n",
        "tokenizer.save_model(\n",
        "    TOKENIZER_LOCATION,\n",
        "    TOKENIZER_NAME\n",
        ")\n",
        "\n",
        "#This is the tokenizer we will use\n",
        "tokenizer = BertTokenizer.from_pretrained(os.path.join(TOKENIZER_LOCATION, TOKENIZER_NAME)+'-vocab.txt')\n",
        "#adding this to handle the inbuilt <unk> token in the dataset which otherwise\n",
        "#gets split into < unk > which may affect (slightly, but still) model performance\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['<unk>']})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkaIC0CR0k8u"
      },
      "source": [
        "\n",
        "All of the input text for BERT training is to be of the form\n",
        "\n",
        "    \"[CLS] <SENTENCE1> [SEP] <SENTENCE2> [SEP]\"\n",
        "with `[PAD]` tokens added at the end as needed.\n",
        "\n",
        "**Notice below how `token_type_ids` denotes where the first sequence ends and the next begins.**\n",
        "\n",
        "Data preprocessing will require us to set up the two  training tasks of NSP and MLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrJwEZxG53xO",
        "outputId": "dceb62bc-d7e9-4b77-edd4-f93dca286147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test=(' = Valkyria Chronicles III = \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n')\n",
            "__input_ids__\n",
            "len=1\n",
            "tensor([[    1,    33,  7754,  7833,  2907,    33,     2,  2957,  5194,   746,\n",
            "          7754,    23,    30, 20395,  7833,    12,  2709,    30,   236,   234,\n",
            "             4,    16,  1244,    18,  7754,   394,   381,  9510,    23,    13,\n",
            "            16,  4627,  3155,   403,   428,  7754,  7833,  2907,  2364,  1826,\n",
            "            16,   445,    42, 10526,  1554,    36,    17,    36,  2202,  1380,\n",
            "           741,  1919,   450,  5160,   248,   399,  2639,    18,  5480,   424,\n",
            "           381,  4501, 10289,    18,  1146,   391,  1425,  1719,   391,  1826,\n",
            "            16,   444,   445,   381,  1361,   741,   391,   381,  7754,   885,\n",
            "            18, 20395,   381,  1145,  8559,   394, 10526,   399,  1511,    36,\n",
            "            17,    36,   689,  5059,   428,   577, 12405,    16,   381,  1422,\n",
            "          3291,  6416,   403,   381,   573,   741,   399,  4397,   381,     6,\n",
            "         18266,     6,    16,    42, 11551,  1594,  3147,  5034,   381,  3659,\n",
            "           394,  3359,   566,   677,   381,   843,  9689,   258,   758,   637,\n",
            "           967,  2461,  1288,  2283,   399,   520, 14511,   384,   972,   381,\n",
            "          5322,  3147,     6, 20395, 18511,     6,    18,     2]])\n",
            "__token_type_ids__\n",
            "len=1\n",
            "tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]])\n",
            "__attention_mask__\n",
            "len=1\n",
            "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "test = (sample[1], sample[3])\n",
        "\n",
        "# test = \"\\n\"\n",
        "print(f\"{test=}\")\n",
        "#automatically adds [CLS] before first sentence and [SEP] after each sentence\n",
        "#this is how Bert separates two sentences in the input\n",
        "tokenized = tokenizer.encode_plus(test[0], test[1], add_special_tokens = True, return_tensors = \"pt\")\n",
        "for key, val in tokenized.items():\n",
        "  print(f\"__{key}__\")\n",
        "  print(f\"len={len(val)}\")\n",
        "  print(val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsHMzNC76hQc"
      },
      "outputs": [],
      "source": [
        "def new_article(para):\n",
        "  #just checks if it is a new article\n",
        "  return para.strip().startswith(\"= \") and not para.strip().startswith(\"= =\")\n",
        "\n",
        "def new_heading(para):\n",
        "  #checks if it is a new heading\n",
        "  #slightly different from the new article\n",
        "  return para.strip().startswith(\"=\")\n",
        "\n",
        "def split_sentences(para):\n",
        "  sentences = para.split('. ')\n",
        "\n",
        "def article_sentence_pairs(article_sentences):\n",
        "  #See note below\n",
        "  sentence_pairs = []\n",
        "  for i, sentence in enumerate(article_sentences):\n",
        "    if i == len(article_sentences)-1:\n",
        "      break\n",
        "    else:\n",
        "      sentence_pairs.append((sentence, article_sentences[i+1]))\n",
        "  return sentence_pairs\n",
        "\n",
        "def collect_sentence_pairs(dataset_iter):\n",
        "  articles = []\n",
        "  current_article = []\n",
        "  for para in tqdm(dataset_iter):\n",
        "    if new_article(para):\n",
        "      #new article, stop collecting sentences\n",
        "      if len(current_article) > 0:\n",
        "        articles += article_sentence_pairs(current_article)\n",
        "      current_article = []\n",
        "      continue\n",
        "    if para.strip() == '' or new_heading(para):\n",
        "      #new heading or empty line, skip and continue collecting\n",
        "      continue\n",
        "    current_article += para.strip().split('. ')\n",
        "  return articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOAOtjrPesbQ"
      },
      "source": [
        "NOTE: there are a few ways we could have split this dataset. I chose to go with one that splits at `\". \"` for this approach but we could have tokenized the data first and split according to maximum sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "67d4fdf26d004eb0a9440ed842f24780",
            "0234d7810df247a5a39d683ce04c6b5d",
            "b9b6b4fe868a4aebae3c240ebd47f20e",
            "776ae0eeb2894a4b85493e23f24e6d7f",
            "a35417309e8240698a8d3b33fdc1dbfe",
            "2318186aa69545f796bf7b5577723cf1",
            "94dd724eb0584a42b6c6ab5916cbf763",
            "73aa2fed59464d21b4beabf6da554c3e",
            "4c6c4db3eb99435cb10ffe2737fba15c",
            "f01205acdc424dcfaafc315defef001f",
            "aa243e66f9914bfda3fa649433e320d5"
          ]
        },
        "id": "URu40H5ReqGc",
        "outputId": "2b3d6427-e0f5-40f2-f92d-b0868d013a6e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67d4fdf26d004eb0a9440ed842f24780"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80096"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "#proper pipeline management, since we iterated once on the old data_train object we reset iteration\n",
        "gen = iter(dataset_class(DATA_DIRECTORY, split = \"train\"))\n",
        "sentence_pairs = collect_sentence_pairs(gen)\n",
        "len(sentence_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmojwWO6lddp",
        "outputId": "8860d864-5bbf-4796-88bd-8dde81768b55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ],
      "source": [
        "sentence_pairs[900000:900010]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Persist sentence pairs to file\n",
        "os.makedirs(os.path.join(DATA_DIRECTORY, \"datasets\", \"pairs_data\"), exist_ok = True)\n",
        "\n",
        "\n",
        "SENTENCE_PAIRS_LOCATION = os.path.join(TOKENS_LOCATION, \"pairs\")\n",
        "os.makedirs(SENTENCE_PAIRS_LOCATION, exist_ok=True)\n",
        "\n",
        "\n",
        "#pickle 200k sentence pairs at a time into various pickle files\n",
        "def pickle_pairs(pairs, chunk_size = 200_000):\n",
        "  i = 0\n",
        "  while chunk_size*(i+1) < len(pairs):\n",
        "    with open(os.path.join(SENTENCE_PAIRS_LOCATION, f\"{i}.pkl\"), \"wb\") as f:\n",
        "      pickle.dump(pairs[chunk_size*i:chunk_size*(i+1)], f)\n",
        "    i+=1\n",
        "\n",
        "pickle_pairs(sentence_pairs)\n"
      ],
      "metadata": {
        "id": "W3HiEHtMj4ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see now that we are able to make pairs of sentences next to one another. Note that our particular method makes it so the first sentence from the immediately following paragraph is treated as a \"next sentence\" but the first sentence of the following article is not."
      ],
      "metadata": {
        "id": "Ra6V0_gbBfky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Two Training Objectives"
      ],
      "metadata": {
        "id": "P2bslE5_CDLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The following examples are straight from the paper and used to illustrate the NSP objective but can be used to explain the MLM objective as well.\n",
        "    \n",
        "    Input: [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
        "    Label: IsNext\n",
        "\n",
        "    Input: [CLS] the man went to [MASK] store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
        "    Label: NotNext"
      ],
      "metadata": {
        "id": "CfRzg63aDJRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Masked Language Model\n",
        "The **Masked Language Model** task masks out a given ratio of the tokens (and with some small probability substitutes with a random token) in the inputs and asks the model to predict the tokens that were masked out."
      ],
      "metadata": {
        "id": "iEACS1_SC14n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Next Sentence Prediction\n",
        "\n",
        "The **Next Sentence Prediction** task takes sentence pairs and creates a balanced classification task, where half of the time the following sentence is actually the next sentence and half of the time it is a random sentence. Then the model is trained to correctly predict whether or not the second sentence is the actual next sentence.\n",
        "\n",
        "We have already done half of the work for this task by creating positive next sentence pairs for each sentence in the text. All we have to do is replace the second sentence in each pair with a random sentence half of the time."
      ],
      "metadata": {
        "id": "mBlyctbQETyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#We will create a custom Dataset class for our purposes\n",
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, sentence_pairs, tokenizer, max_len=512):\n",
        "    self.sentence_pairs = sentence_pairs\n",
        "    self.num_pairs = len(sentence_pairs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.num_pairs\n",
        "\n",
        "  #The key component of a Dataset is the __getitem__ function\n",
        "  def __getitem__(self, idx):\n",
        "    #get nsp entry, encode, return\n",
        "    sentence1, sentence2, isNextLabel = self.get_nsp_entry(idx)\n",
        "    encoded_pair = tokenizer.encode_plus(\n",
        "        sentence1,\n",
        "        sentence2,\n",
        "        add_special_tokens = True,\n",
        "        max_length = self.max_len,\n",
        "        padding = 'max_length',\n",
        "        truncation = 'longest_first',\n",
        "        return_special_tokens_mask = True,\n",
        "        return_tensors = \"pt\"\n",
        "    )\n",
        "\n",
        "    return encoded_pair, isNextLabel\n",
        "\n",
        "\n",
        "  def get_nsp_entry(self, idx):\n",
        "    #Implement NSP randomization here\n",
        "    sent1, sent2 = self.sentence_pairs[idx]\n",
        "    if random.random() >= 0.5:\n",
        "      #this is the case where we give positive nsp example\n",
        "      return sent1, sent2, 1\n",
        "    else:\n",
        "      return sent1, self.get_non_next_sentence(idx), 0\n",
        "\n",
        "  def get_non_next_sentence(self, idx):\n",
        "    random_idx = random.randrange(self.num_pairs)\n",
        "    while random_idx == idx:\n",
        "      \"\"\"\n",
        "      this is just here for the small chance that\n",
        "      our random index maps to the same one and gives\n",
        "      us a false pair where the actual next sentence\n",
        "      is mislabeled as NotNext\n",
        "      \"\"\"\n",
        "      random_idx = random.randrange(self.num_pairs)\n",
        "    return self.sentence_pairs[random_idx][1]\n",
        "\n"
      ],
      "metadata": {
        "id": "lvIguW-eBfVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Example using the Transformers Library for MLM\n",
        "\n",
        "We can use the `transformers.DataCollatorForLanguageModeling` class to handle the masking for us.\n",
        "\n",
        "We see [in the documentation](https://github.com/huggingface/transformers/blob/b71f20a7c9f3716d30f6738501559acf863e2c5c/src/transformers/data/data_collator.py#L751C1-L751C108) that exactly like in the paper, `DataCollatorForLanguageModeling` will mask tokens 80% of the time, replace with a random token 10% of the time, and leave as is 10% of the time. This is all conditional on the `mlm_probability` value passed into the collator which is `0.15` in the paper."
      ],
      "metadata": {
        "id": "EBZIiacqlQPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = True, mlm_probability = 0.15)"
      ],
      "metadata": {
        "id": "kk9NJ5exV-9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We recreate our tokenization example from earlier,\n",
        "#we just want to create a sentence pair for illustration\n",
        "feats = tokenizer.encode_plus(\n",
        "    sample[1],\n",
        "    sample[3],\n",
        "    return_special_tokens_mask = True,\n",
        "    return_tensors = \"pt\",\n",
        "    max_length = 512,\n",
        "    truncation=True,\n",
        "    padding = 'max_length'\n",
        ")\n"
      ],
      "metadata": {
        "id": "W4t3AtDVcJ7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feats"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xPn9UBJTAHc",
        "outputId": "592a16fe-d797-4104-d291-527e679bdb98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[    1,    33,  7754,  7833,  2907,    33,     2,  2957,  5194,   746,\n",
              "          7754,    23,    30, 20395,  7833,    12,  2709,    30,   236,   234,\n",
              "             4,    16,  1244,    18,  7754,   394,   381,  9510,    23,    13,\n",
              "            16,  4627,  3155,   403,   428,  7754,  7833,  2907,  2364,  1826,\n",
              "            16,   445,    42, 10526,  1554,    36,    17,    36,  2202,  1380,\n",
              "           741,  1919,   450,  5160,   248,   399,  2639,    18,  5480,   424,\n",
              "           381,  4501, 10289,    18,  1146,   391,  1425,  1719,   391,  1826,\n",
              "            16,   444,   445,   381,  1361,   741,   391,   381,  7754,   885,\n",
              "            18, 20395,   381,  1145,  8559,   394, 10526,   399,  1511,    36,\n",
              "            17,    36,   689,  5059,   428,   577, 12405,    16,   381,  1422,\n",
              "          3291,  6416,   403,   381,   573,   741,   399,  4397,   381,     6,\n",
              "         18266,     6,    16,    42, 11551,  1594,  3147,  5034,   381,  3659,\n",
              "           394,  3359,   566,   677,   381,   843,  9689,   258,   758,   637,\n",
              "           967,  2461,  1288,  2283,   399,   520, 14511,   384,   972,   381,\n",
              "          5322,  3147,     6, 20395, 18511,     6,    18,     2,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]]), 'special_tokens_mask': tensor([[1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked = collator([feats])\n",
        "masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bq17uOXaunC",
        "outputId": "5ebd0add-4a67-4294-d2f2-f092f8314392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[[    1,    33,  7754,  7833,  2907,     3,     2,  2957,  5194,   746,\n",
              "           7754,    23,    30, 20395,     3,    12,  2709,    30,   236,   234,\n",
              "              3,     3,  1244,     3,  7754,   394,     3,  9510,    23,    13,\n",
              "             16,  4627,  3155,   403,     3,     3,  7833,  2907,  2364,  1826,\n",
              "             16,   445,    42,     3,  1554,     3,    17,    36,  2202,  1380,\n",
              "            741,  1919,   450,  5160,   248,   399,  2639,    18,     3,     3,\n",
              "            381,     3, 10289,    18,  1146,   391,     3,  1719,   391,  1826,\n",
              "             16,   444,   445,   381,     3,   741,   391,   381,  7754,   885,\n",
              "             18, 20395,   381,  1145,  8559,   394, 10526,   399,  1511,    36,\n",
              "             17,     3,   689,  5059,   428,   577, 12405,     3,   381,  1422,\n",
              "           3291,  6416,   403,   381,   573, 16518,   399,  4397,   381,     6,\n",
              "          18266,     6,    16,    42, 11551,  1594,  3147,  5034,   381,  3659,\n",
              "            394,  3359,     3,   677,   381,   843,  9689,   258,   758,   637,\n",
              "              3,  2461,  1288,  2283,     3,   520, 14511,   384,   972,   381,\n",
              "           5322,  3147,     6, 20395, 18511,     6, 18103,     2,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0]]]), 'token_type_ids': tensor([[[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0]]]), 'attention_mask': tensor([[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0]]]), 'labels': tensor([[[ -100,  -100,  -100,  -100,  -100,    33,  -100,  -100,  -100,  -100,\n",
              "           7754,  -100,  -100,  -100,  7833,  -100,  -100,  -100,  -100,  -100,\n",
              "              4,    16,  -100,    18,  -100,  -100,   381,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,   403,   428,  7754,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100, 10526,  -100,    36,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  5480,   424,\n",
              "           -100,  4501,  -100,  -100,  -100,  -100,  1425,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  1361,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,    36,  -100,  -100,  -100,  -100,  -100,    16,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,   573,   741,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,   566,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "            967,  -100,  -100,  -100,   399,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,    18,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100]]])}"
            ]
          },
          "metadata": {},
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(masked['input_ids'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "zcGK-zY3kLp0",
        "outputId": "5de66e0d-c146-4475-e0e9-5b4d8bdd5b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] = valkyria chronicles iii [MASK] [SEP] senjo no valkyria 3 : <unk> [MASK] ( japanese : 戦 場 [MASK] [MASK] lit [MASK] valkyria of [MASK] battlefield 3 ), commonly referred to [MASK] [MASK] chronicles iii outside japan, is a [MASK] role [MASK] - @ playing video game developed by sega and media. [MASK] [MASK] the [MASK] portable. released in [MASK] 2011 in japan, it is the [MASK] game in the valkyria series. <unk> the same fusion of tactical and real @ - [MASK] time gameplay as its predecessors [MASK] the story runs parallel to the first octagonal and follows the \" nameless \", a penal military unit serving the nation of gall [MASK] during the second europan war who [MASK] secret black operations [MASK] are pitted against the imperial unit \" <unk> raven \" careful [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 182
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(feats['input_ids'].squeeze())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "3CwE_vdzkuST",
        "outputId": "cbd79f27-c8de-4d14-f92d-1bdb94ce91d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] = valkyria chronicles iii = [SEP] senjo no valkyria 3 : <unk> chronicles ( japanese : 戦 場 [UNK], lit. valkyria of the battlefield 3 ), commonly referred to as valkyria chronicles iii outside japan, is a tactical role @ - @ playing video game developed by sega and media. vision for the playstation portable. released in january 2011 in japan, it is the third game in the valkyria series. <unk> the same fusion of tactical and real @ - @ time gameplay as its predecessors, the story runs parallel to the first game and follows the \" nameless \", a penal military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" <unk> raven \". [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 183
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rorGbQJC29w5"
      },
      "source": [
        "##Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will create the architecture of the model from scratch. I will not go into too much detail about it since it is so similar to the original transformer encoder (which I mentioned above at the beginning of this notebook). But there will be some helpful annotations and comments."
      ],
      "metadata": {
        "id": "2gnlKxTaVp0E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Embedding"
      ],
      "metadata": {
        "id": "EFmdPpc5BpHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer Encoder (the decoder as well, for that matter) uses a Positional Embedding to provide information about the positions of tokens in the sequence. This is necessary because attention is bidirectional and has no way to account for absolute position of tokens in a sequence on its own.\n",
        "\n",
        "We also have to find a way to include information about the two segments (since we pass in pairs of sentences). Our tokenizer provides information about the segments in a vector of 0s and 1s as per the paper specs. We add an embedding layer that takes two tokens in and outputs and embedding of the same dimension as our token embedding.\n",
        "\n",
        "This segment embedding represents one of the few differences between the model architecture in BERT and the original Transformer."
      ],
      "metadata": {
        "id": "66IptYXJBm0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "  def __init__(self, d_model, dropout = 0.0, max_len=512):\n",
        "    super(PositionalEncoding, self).__init__()\n",
        "    self.dropout = nn.Dropout(p=dropout)\n",
        "    self.d_model = d_model\n",
        "    self.max_len = max_len\n",
        "\n",
        "    #we can just compute the tensor for PE ahead of time\n",
        "    self.pe = torch.zeros(max_len, d_model).float()\n",
        "    self.pe.requires_grad = False\n",
        "\n",
        "    for position in range(max_len):\n",
        "      for i in range(0, d_model, 2):\n",
        "        self.pe[position, i] = math.sin(position / (10000 ** ((2 * i)/d_model)))\n",
        "        self.pe[position, i + 1] = math.cos(position / (10000 ** ((2 * (i + 1))/d_model)))\n",
        "\n",
        "    self.pe = self.pe.unsqueeze(0)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.pe[ : , : x.size(1)]\n",
        "    return self.dropout(x)\n",
        "\n",
        "\n",
        "class BERTEmbeddingLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  The essential steps of embedding here are:\n",
        "  Embed tokens to our chosen embedding_dim\n",
        "  Add positional encoding to the base embedding\n",
        "  Add segment embeddings (to represent which sentence of two each token is part of)\n",
        "\n",
        "  Note there are only two values for segment labels.\n",
        "  The padding tokens after the second segment are labeled 0 just like the first.\n",
        "  \"\"\"\n",
        "  def __init__(self, vocab_size, embedding_dim, seq_len = 512, dropout = 0.0):\n",
        "    super(BERTEmbeddingLayer, self).__init__()\n",
        "    self.positional_encoding = PositionalEncoding(embedding_dim, dropout, seq_len)\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = 0)\n",
        "    self.segment_embedding = nn.Embedding(2, embedding_dim)\n",
        "\n",
        "  def forward(self, x, segment_labels):\n",
        "    return self.token_embedding(x) + self.segment_embedding(segment_labels) + self.positional_encoding(x)\n"
      ],
      "metadata": {
        "id": "rzXhBU7GUMKc"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder Architecture"
      ],
      "metadata": {
        "id": "9LG63m-UBgMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Transformer Encoder (and BERT, by extension) only uses self-attention in a manner where `query`, `key`, and `value` are all the same since we don't employ an \"encoder memory\" anywhere the way we would with the decoder.\n",
        "\n",
        "In the `BERT_base` model, there are 12 instead of 6 stacked blocks, `d_model = 768`, there are 12 heads of attention. These represent some of the differences in configuration between the original Transformer and BERT encoders but aren't actual architecture differences.\n",
        "\n",
        "Another minor difference is that the feedforward activation function is chosen to be `gelu` instead of `relu`."
      ],
      "metadata": {
        "id": "9w_EkECKEShg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clone_layers = lambda layer, n: nn.ModuleList([copy.deepcopy(layer) for _ in range(n)])"
      ],
      "metadata": {
        "id": "in3W3HqfQ2y-"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement MultiHeadAttention\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_model, num_heads, dropout = 0.1):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.d_model = d_model\n",
        "    self.d_k = d_model // num_heads\n",
        "    self.num_heads = num_heads\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "    self.query, self.key, self.value, self.out = clone_layers(nn.Linear(d_model, d_model), 4)\n",
        "\n",
        "  def forward(self, query, key, value, mask):\n",
        "    #The shapes of these tensors are annotated for easier intuition\n",
        "    #b = batch_size, s = seq_len, h = num_heads\n",
        "    #(b, s, d_model)\n",
        "    Q = self.query(query)\n",
        "    K = self.key(key)\n",
        "    V = self.value(value)\n",
        "\n",
        "    #(b, s, d_model) -> (b, s, h, d_k) -> (b, h, s, d_k)\n",
        "    Q = Q.view(Q.size(0), -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    K = K.view(K.size(0), -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "    V = V.view(V.size(0), -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    #Note that MHA does Scaled Dot Product attention over each head\n",
        "    #and then concatenates. So scores is actually done for each head.\n",
        "    #(b, h, s, d_k) x (b, h, d_k, s) -> scalar * (b, h, s, s)\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "\n",
        "    #fill masked scores with low value (imagine using -inf)\n",
        "    #to minimize impact on softmax output\n",
        "    #fill does not change shape\n",
        "    #(b, h, s, s)\n",
        "    scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    #softmax does not change shape\n",
        "    #(b, h, s, s)\n",
        "    weights = F.softmax(scores, dim = -1)\n",
        "    weights = self.dropout(weights)\n",
        "\n",
        "    #(b, h, s, s) x (b, h, s, d_k) -> (b, h, s, d_k)\n",
        "    #this would be the memory or context in transformer\n",
        "    memory = torch.matmul(weights, V)\n",
        "\n",
        "    #(b, h, s, d_k) -> (b, s, h, d_k) -> (b, s, h * d_k) = (b, s, d_model)\n",
        "    #back to original shape\n",
        "    memory = memory.transpose(1, 2).contiguous().view(memory.size(0), -1, self.num_heads * self.d_k)\n",
        "\n",
        "    #(b, s, d_model) -> (b, s, d_model)\n",
        "    return self.out(memory)\n",
        ""
      ],
      "metadata": {
        "id": "h7UNUT4WBfkF"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original BERT paper maintains the relationship between `d_model` and `d_ff` in the feedforward layer where `d_ff = 4 * d_model`."
      ],
      "metadata": {
        "id": "qiHOciobSRUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Implement FF Layer for Encoder\n",
        "class FeedForwardLayer(nn.Module):\n",
        "  def __init__(self, d_model, d_ff = 3072, dropout = 0.0):\n",
        "    #Note in the BERT paper, BERT_base has d_ff = 3072 = 4 * d_model = 4 * 768\n",
        "    super(FeedForwardLayer, self).__init__()\n",
        "    self.linear1 = nn.Linear(d_model, d_ff)\n",
        "    self.linear2 = nn.Linear(d_ff, d_model)\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "    self.activation = nn.GELU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear2(self.dropout(self.activation(self.linear1(x))))"
      ],
      "metadata": {
        "id": "u8oBjMop_0C1"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we implement the EncoderLayer. We need to use layernormalization here."
      ],
      "metadata": {
        "id": "VTRPh6OBTZ14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#All default values are selected based on BERT_base in the paper\n",
        "class EncoderLayer(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      d_model=768,\n",
        "      num_heads=12,\n",
        "      d_ff = 3072,\n",
        "      dropout = 0.1\n",
        "    ):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "    self.ff = FeedForwardLayer(d_model, d_ff, dropout)\n",
        "    self.norm = nn.LayerNorm(d_model)\n",
        "    self.dropout = nn.Dropout(p = dropout)\n",
        "\n",
        "  def forward(self, embeddings, mask):\n",
        "    #Shapes: b = batch_size, s = seq_len\n",
        "    #embeddings: (b, s, d_model)\n",
        "    #mask: (b, 1, 1, s)\n",
        "\n",
        "    #attention_out: (b, s, d_model)\n",
        "    attention_out = self.dropout(self.mha(embeddings, embeddings, embeddings, mask))\n",
        "\n",
        "    #attention_out: (b, s, d_model)\n",
        "    attention_out = self.norm(attention_out+embeddings)\n",
        "\n",
        "    #ff_out: (b, s, d_model)\n",
        "    ff_out = self.dropout(self.ff(attention_out))\n",
        "\n",
        "    return self.norm(ff_out + attention_out)\n"
      ],
      "metadata": {
        "id": "mAUQAf73Tmsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BERT Model"
      ],
      "metadata": {
        "id": "x8f4YBbwWTPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we implement the model by putting together the above classes we created."
      ],
      "metadata": {
        "id": "9Gt-EfNhWaz5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "  def __init__(\n",
        "      self,\n",
        "      vocab_size,\n",
        "      d_model = 768,\n",
        "      blocks = 12,\n",
        "      num_heads = 12,\n",
        "      seq_len = 512,\n",
        "      dropout = 0.1\n",
        "    ):\n",
        "    super(BERT, self).__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.d_model = d_model\n",
        "    self.d_ff = 4 * d_model #as noted above, ff hidden layer dim is 4*d_model\n",
        "    self.blocks = blocks\n",
        "    self.num_heads = num_heads\n",
        "    self.seq_len = seq_len\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.embedding = BERTEmbeddingLayer(\n",
        "        self.vocab_size,\n",
        "        self.d_model,\n",
        "        seq_len = self.seq_len,\n",
        "        dropout = self.dropout\n",
        "    )\n",
        "\n",
        "    self.encoder_blocks = clone_layers(EncoderLayer(d_model, num_heads, self.d_ff, dropout), blocks)\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "5lIeoM2aWezx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prBAUhze37Qv"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MakZRzAE6ESt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyObIorqkh1LSFZEP6+FTBaf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67d4fdf26d004eb0a9440ed842f24780": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0234d7810df247a5a39d683ce04c6b5d",
              "IPY_MODEL_b9b6b4fe868a4aebae3c240ebd47f20e",
              "IPY_MODEL_776ae0eeb2894a4b85493e23f24e6d7f"
            ],
            "layout": "IPY_MODEL_a35417309e8240698a8d3b33fdc1dbfe"
          }
        },
        "0234d7810df247a5a39d683ce04c6b5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2318186aa69545f796bf7b5577723cf1",
            "placeholder": "​",
            "style": "IPY_MODEL_94dd724eb0584a42b6c6ab5916cbf763",
            "value": ""
          }
        },
        "b9b6b4fe868a4aebae3c240ebd47f20e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73aa2fed59464d21b4beabf6da554c3e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c6c4db3eb99435cb10ffe2737fba15c",
            "value": 1
          }
        },
        "776ae0eeb2894a4b85493e23f24e6d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f01205acdc424dcfaafc315defef001f",
            "placeholder": "​",
            "style": "IPY_MODEL_aa243e66f9914bfda3fa649433e320d5",
            "value": " 36718/? [00:00&lt;00:00, 69651.23it/s]"
          }
        },
        "a35417309e8240698a8d3b33fdc1dbfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2318186aa69545f796bf7b5577723cf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94dd724eb0584a42b6c6ab5916cbf763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "73aa2fed59464d21b4beabf6da554c3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4c6c4db3eb99435cb10ffe2737fba15c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f01205acdc424dcfaafc315defef001f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa243e66f9914bfda3fa649433e320d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}