{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/word2vec-and-BERT/blob/main/BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ursbIzV2v9p7"
      },
      "source": [
        "#BERT (Bidirectional Encoder Representations from Transformers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hEcTXgYwL-N"
      },
      "source": [
        "In this notebook I will be replicating the model in the paper [**BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805.pdf).\n",
        "\n",
        "While I will be creating the model from (mostly) scratch in PyTorch, I will not go into too much detail about why Multi-Head Attention is designed the way it is and how exactly the original [Transformer](https://arxiv.org/abs/1706.03762) architecture works. I have made another (*albeit messy*) [notebook that covers that paper](https://github.com/jaroorhmodi/transformer-from-scratch).\n",
        "\n",
        "The model will be trained on the [**WikiText-2**](https://paperswithcode.com/dataset/wikitext-2) and [**Wikitext-103**](https://paperswithcode.com/dataset/wikitext-103) datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Koq_QwuW4MbN",
        "outputId": "b5d0cf18-ad5e-49d7-8cf9-9e0c18ed79b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.8.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install portalocker transformers tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWkkb9uc4T5B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103 #our datasets for this project\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "DATASET_small = \"WikiText2\"\n",
        "DATASET_large = \"WikiText103\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "TOKENIZER=\"basic_english\"\n",
        "DATA_DIRECTORY = \"/content\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09DBB2wd24DJ"
      },
      "source": [
        "##Model Objective and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sVv47JM5I7p"
      },
      "source": [
        "###Who (What) is BERT?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCHzPRMbL6Pw"
      },
      "source": [
        "\n",
        "While BERT is a much more complex model and what it accomplishes isn't exactly akin to Word2Vec, the intuition behind both is similar. We pass in sentences and attempt to make a model learn how to represent text in a way that captures not only information about the tokens themselves but also something about their *meaning*.\n",
        "\n",
        "Word2Vec does this by training a model on words and their context in sentences and learning  about their relationships with one another by either trying to predict context from words (*Skip-Gram*) or words from context (*CBOW*). The embeddings it produces are static for each word.\n",
        "\n",
        "BERT trains a Transformer Encoder model on two specific objectives: *Masked Language Modeling* and *Next Sentence Prediction* to learn a wealth of information about tokens in their context and provide representations of them. Note that BERT is not simply learning static embeddings but rather representations that change based on context. Tokens in BERT are embedded using *WordPiece* embeddings.\n",
        "\n",
        "The goal of the BERT paper was to introduce a way to represent words with a pre-trained transformer and not to make a model for a specific predictive goal. To this end it is trained in an unsupervised manner with the aforementioned MLM and NSP objectives (will be explained ahead)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZh1_Ky2gPKa"
      },
      "source": [
        "###Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjWVBG8Ff9jY"
      },
      "outputs": [],
      "source": [
        "#We need to pull in the dataset and break it into sentence pairs for the NSP objective\n",
        "#and we need to mask random words and create objectives for the MLM objective.\n",
        "DATASET = DATASET_small\n",
        "dataset_class = WikiText2 if DATASET == DATASET_small else WikiText103\n",
        "data_train = dataset_class(DATA_DIRECTORY, split = \"train\")\n",
        "TOKENS_LOCATION = os.path.join(DATA_DIRECTORY, \"datasets\", DATASET, DATASET.lower()[:8]+f\"-{DATASET[8:]}\", \"wiki.train.tokens\")\n",
        "\n",
        "MAX_TOKENIZED_SENTENCE_LEN = 128 #maximum number of tokens in sentence\n",
        "\n",
        "gen = iter(data_train)\n",
        "\n",
        "sample = []\n",
        "for i in range(20):\n",
        "  sample.append(next(gen))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVIW8MfNiYX-",
        "outputId": "d6ab0849-49b6-4fb9-bcdc-c44798a7a8ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\"]\n",
            "20\n"
          ]
        }
      ],
      "source": [
        "print(sample[:5])\n",
        "print(len(sample))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tx3b0r8KkA8x"
      },
      "source": [
        "In the WikiText data, we see a lot of control characters like newlines, we want to make sure when we tokenize that we do not consider these. We also see that articles are delineated by a header given between single **=** signs and subheaders given by double, triple etc. equal signs.\n",
        "\n",
        "We don't want header paragraphs and we don't want empty whitespace lines, so we need to preprocess the data some before we actually split them out into sentence pairs for our NSP task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXKcxThinJxC",
        "outputId": "9c349694-b011-402c-cbf4-55b9ec9125da"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/tokenizers/bert-wordpiece-WikiText2-vocab.txt']"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text = True, #removes control chars like \\n\n",
        "    handle_chinese_chars = False, #not anticipating chinese chars\n",
        "    strip_accents = False, #keep accents in\n",
        "    lowercase = True #ignore case\n",
        ")\n",
        "\n",
        "tokenizer.train(\n",
        "    files = TOKENS_LOCATION,\n",
        "    vocab_size = 30_000,\n",
        "    min_frequency = 10,\n",
        "    limit_alphabet = 1000,\n",
        "    wordpieces_prefix = '##',\n",
        "    special_tokens=['[UNK]']\n",
        ")\n",
        "\n",
        "os.makedirs(os.path.join(DATA_DIRECTORY, \"tokenizers\"), exist_ok = True)\n",
        "\n",
        "tokenizer.save_model(\n",
        "    os.path.join(DATA_DIRECTORY, \"tokenizers\"),\n",
        "    f\"bert-wordpiece-{DATASET}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yrJwEZxG53xO",
        "outputId": "862961cf-33a6-461f-aff2-629c0f2e8ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test=[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n', ' Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n']\n",
            "example.tokens=[]\n",
            "example.ids=[]\n",
            "example.offsets=[]\n",
            "example.tokens=['=', 'valkyria', 'chronicles', 'iii', '=']\n",
            "example.ids=[29, 7750, 7829, 2903, 29]\n",
            "example.offsets=[(1, 2), (3, 11), (12, 22), (23, 26), (27, 28)]\n",
            "example.tokens=[]\n",
            "example.ids=[]\n",
            "example.offsets=[]\n",
            "example.tokens=['sen', '##j', '##ō', 'no', 'valkyria', '3', ':', '<', 'unk', '>', 'chronicles', '(', 'japanese', ':', '戦', '##場', '##の', '##ヴ', '##ァ', '##ル', '##キ', '##ュ', '##リ', '##ア', '##3', ',', 'lit', '.', 'valkyria', 'of', 'the', 'battlefield', '3', ')', ',', 'commonly', 'referred', 'to', 'as', 'valkyria', 'chronicles', 'iii', 'outside', 'japan', ',', 'is', 'a', 'tactical', 'role', '@', '-', '@', 'playing', 'video', 'game', 'developed', 'by', 'seg', '##a', 'and', 'media', '.', 'vision', 'for', 'the', 'playstation', 'portable', '.', 'released', 'in', 'january', '2011', 'in', 'japan', ',', 'it', 'is', 'the', 'third', 'game', 'in', 'the', 'valkyria', 'series', '.', '<', 'unk', '>', 'the', 'same', 'fusion', 'of', 'tactical', 'and', 'real', '@', '-', '@', 'time', 'gameplay', 'as', 'its', 'predecessors', ',', 'the', 'story', 'runs', 'parallel', 'to', 'the', 'first', 'game', 'and', 'follows', 'the', '\"', 'nameless', '\"', ',', 'a', 'penal', 'military', 'unit', 'serving', 'the', 'nation', 'of', 'gall', '##ia', 'during', 'the', 'second', 'europa', '##n', 'war', 'who', 'perform', 'secret', 'black', 'operations', 'and', 'are', 'pitt', '##ed', 'against', 'the', 'imperial', 'unit', '\"', '<', 'unk', '>', 'raven', '\"', '.']\n",
            "example.ids=[2953, 254, 292, 742, 7750, 19, 26, 28, 392, 30, 7829, 8, 2705, 26, 232, 348, 349, 350, 351, 352, 353, 354, 355, 356, 268, 12, 1240, 14, 7750, 390, 377, 9506, 19, 9, 12, 4623, 3151, 399, 424, 7750, 7829, 2903, 2360, 1822, 12, 441, 38, 10522, 1550, 32, 13, 32, 2198, 1376, 737, 1915, 446, 5156, 243, 395, 2635, 14, 5476, 420, 377, 4497, 10285, 14, 1142, 387, 1421, 1715, 387, 1822, 12, 440, 441, 377, 1357, 737, 387, 377, 7750, 881, 14, 28, 392, 30, 377, 1141, 8555, 390, 10522, 395, 1507, 32, 13, 32, 685, 5055, 424, 573, 12401, 12, 377, 1418, 3287, 6412, 399, 377, 569, 737, 395, 4393, 377, 2, 18262, 2, 12, 38, 11547, 1590, 3143, 5030, 377, 3655, 390, 3355, 562, 673, 377, 839, 9685, 253, 754, 633, 963, 2457, 1284, 2279, 395, 516, 14507, 380, 968, 377, 5318, 3143, 2, 28, 392, 30, 18507, 2, 14]\n",
            "example.offsets=[(1, 4), (4, 5), (5, 6), (7, 9), (10, 18), (19, 20), (21, 22), (23, 24), (24, 27), (27, 28), (29, 39), (40, 41), (42, 50), (51, 52), (53, 54), (54, 55), (55, 56), (56, 57), (57, 58), (58, 59), (59, 60), (60, 61), (61, 62), (62, 63), (63, 64), (65, 66), (67, 70), (71, 72), (73, 81), (82, 84), (85, 88), (89, 100), (101, 102), (103, 104), (105, 106), (107, 115), (116, 124), (125, 127), (128, 130), (131, 139), (140, 150), (151, 154), (155, 162), (163, 168), (169, 170), (171, 173), (174, 175), (176, 184), (185, 189), (190, 191), (191, 192), (192, 193), (194, 201), (202, 207), (208, 212), (213, 222), (223, 225), (226, 229), (229, 230), (231, 234), (235, 240), (240, 241), (241, 247), (248, 251), (252, 255), (256, 267), (268, 276), (277, 278), (279, 287), (288, 290), (291, 298), (299, 303), (304, 306), (307, 312), (313, 314), (315, 317), (318, 320), (321, 324), (325, 330), (331, 335), (336, 338), (339, 342), (343, 351), (352, 358), (359, 360), (361, 362), (362, 365), (365, 366), (367, 370), (371, 375), (376, 382), (383, 385), (386, 394), (395, 398), (399, 403), (404, 405), (405, 406), (406, 407), (408, 412), (413, 421), (422, 424), (425, 428), (429, 441), (442, 443), (444, 447), (448, 453), (454, 458), (459, 467), (468, 470), (471, 474), (475, 480), (481, 485), (486, 489), (490, 497), (498, 501), (502, 503), (504, 512), (513, 514), (515, 516), (517, 518), (519, 524), (525, 533), (534, 538), (539, 546), (547, 550), (551, 557), (558, 560), (561, 565), (565, 567), (568, 574), (575, 578), (579, 585), (586, 592), (592, 593), (594, 597), (598, 601), (602, 609), (610, 616), (617, 622), (623, 633), (634, 637), (638, 641), (642, 646), (646, 648), (649, 656), (657, 660), (661, 669), (670, 674), (675, 676), (677, 678), (678, 681), (681, 682), (683, 688), (689, 690), (691, 692)]\n"
          ]
        }
      ],
      "source": [
        "test = sample[:4]\n",
        "# test = \"\\n\"\n",
        "print(f\"{test=}\")\n",
        "tokenized = tokenizer.encode_batch(test)\n",
        "for i in range(len(tokenized)):\n",
        "  example = tokenized[i]\n",
        "  print(f\"{example.tokens=}\")\n",
        "  print(f\"{example.ids=}\")\n",
        "  print(f\"{example.offsets=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkaIC0CR0k8u"
      },
      "source": [
        "Data preprocessing will require us to satisfy the two aforementioned training tasks.\n",
        "\n",
        "All of the input text for BERT training is to be of the form\n",
        "\n",
        "    \"[CLS] <SENTENCE1> [SEP] <SENTENCE2> [SEP]\"\n",
        "with `[PAD]` tokens added at the end as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me1DnxBU8Dfv",
        "outputId": "8cfbfbb8-bcfa-4155-9ca5-1245af74290a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample[0].strip() == ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "CsHMzNC76hQc"
      },
      "outputs": [],
      "source": [
        "def new_article(para):\n",
        "  #just checks if it is a new article\n",
        "  return para.strip().startswith(\"= \") and not para.strip().startswith(\"= =\")\n",
        "\n",
        "def new_heading(para):\n",
        "  #checks if it is a new heading\n",
        "  #slightly different from the new article\n",
        "  return para.strip().startswith(\"=\")\n",
        "\n",
        "def split_sentences(para):\n",
        "  sentences = para.split('. ')\n",
        "\n",
        "def article_sentence_pairs(article_sentences):\n",
        "  #See note below\n",
        "  sentence_pairs = []\n",
        "  for i, sentence in enumerate(article_sentences):\n",
        "    if i == len(article_sentences)-1:\n",
        "      break\n",
        "    else:\n",
        "      sentence_pairs.append((sentence, article_sentences[i+1]))\n",
        "  return sentence_pairs\n",
        "\n",
        "def collect_sentence_pairs(dataset_iter):\n",
        "  articles = []\n",
        "  current_article = []\n",
        "  for para in dataset_iter:\n",
        "    if new_article(para):\n",
        "      #new article, stop collecting sentences\n",
        "      if len(current_article) > 0:\n",
        "        articles += article_sentence_pairs(current_article)\n",
        "      current_article = []\n",
        "      continue\n",
        "    if para.strip() == '' or new_heading(para):\n",
        "      #new heading or empty line, skip and continue collecting\n",
        "      continue\n",
        "    current_article += para.strip().split('. ')\n",
        "  return articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOAOtjrPesbQ"
      },
      "source": [
        "NOTE: there are a few ways we could have split this dataset. I chose to go with one that splits at `\". \"` for this approach but we could have tokenized the data first and split according to maximum sequence length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URu40H5ReqGc",
        "outputId": "2dd81350-5ca0-40c7-8301-82bca7513695"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80096"
            ]
          },
          "metadata": {},
          "execution_count": 136
        }
      ],
      "source": [
        "gen = iter(data_train)\n",
        "sentence_pairs = collect_sentence_pairs(gen)\n",
        "len(sps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmojwWO6lddp",
        "outputId": "0aca7691-cbb9-48b4-e780-719736965231"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"The rejected main theme was used as a hopeful tune that played during the game 's ending \",\n",
              "  'The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of <unk> '),\n",
              " ('The battle themes were designed around the concept of a \" modern battle \" divorced from a fantasy scenario by using modern musical instruments , constructed to create a sense of <unk> ',\n",
              "  'While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar '),\n",
              " ('While Sakimoto was most used to working with synthesized music , he felt that he needed to incorporate live instruments such as orchestra and guitar ',\n",
              "  'The guitar was played by <unk> <unk> , who also arranged several of the later tracks ')]"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ],
      "source": [
        "sentence_pairs[77:80]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see now that we are able to make pairs of sentences next to one another. Note that our particular method makes it so the first sentence from the immediately following paragraph is treated as a \"next sentence\" but the first sentence of the following article is not."
      ],
      "metadata": {
        "id": "Ra6V0_gbBfky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Two Training Objectives"
      ],
      "metadata": {
        "id": "P2bslE5_CDLv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  The following examples are straight from the paper and used to illustrate the NSP objective but can be used to explain the MLM objective as well.\n",
        "    \n",
        "    Input: [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP]\n",
        "    Label: IsNext\n",
        "\n",
        "    Input: [CLS] the man went to [MASK] store [SEP] penguin [MASK] are flight ##less birds [SEP]\n",
        "    Label: NotNext"
      ],
      "metadata": {
        "id": "CfRzg63aDJRV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Masked Language Model\n",
        "The **Masked Language Model** task masks out a given ratio of the tokens (and with some small probability substitutes with a random token) in the inputs and asks the model to predict the tokens that were masked out."
      ],
      "metadata": {
        "id": "iEACS1_SC14n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Next Sentence Prediction\n",
        "\n",
        "The **Next Sentence Prediction** task takes sentence pairs and creates a balanced classification task, where half of the time the following sentence is actually the next sentence and half of the time it is a random sentence. Then the model is trained to correctly predict whether or not the second sentence is the actual next sentence."
      ],
      "metadata": {
        "id": "mBlyctbQETyg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lvIguW-eBfVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiC9ezXxmFY4",
        "outputId": "74fa1f59-1624-4c35-9153-9fd402094468"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rorGbQJC29w5"
      },
      "source": [
        "##Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PD18JF5qvtId"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prBAUhze37Qv"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MakZRzAE6ESt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFVvCTKiTH5Z75cJaCdnzC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}