{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fmodern\fcharset0 Courier;}
{\colortbl;\red255\green255\blue255;\red245\green245\blue245;\red18\green112\blue68;\red15\green112\blue1;
\red0\green0\blue255;}
{\*\expandedcolortbl;;\cssrgb\c96863\c96863\c96863;\cssrgb\c3529\c50588\c33725;\cssrgb\c0\c50196\c0;
\cssrgb\c0\c0\c100000;}
\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs28 \cf0 \cb2 \expnd0\expndtw0\kerning0
EMBEDDING_DIM = \cf3 300\cf0  \cf4 #in the paper the recommended dimension is 300\cf0 \cb1 \
\cb2 EMBEDDING_MAX_NORM = \cf3 1\cf0  \cf4 #make sure embedding vectors are no longer than unit vectors\cf0 \cb1 \
\
\cb2 NUM_EPOCHS = \cf3 50\cf0  \cf4 #number of epochs in training\cf0 \cb1 \
\cb2 REPORT_EPOCHS = \cf3 5\cf0  \cf4 #print losses every REPORT_EPOCHS epochs\cf0 \cb1 \
\cb2 TRAIN_BATCH_SIZE = \cf3 128\cf0 \cb1 \
\cb2 VAL_BATCH_SIZE = \cf3 128\cf0 \cb1 \
\
\cb2 LR = \cf3 1\cf0 \cb1 \
\
\cb2 torch.manual_seed(\cf3 2118\cf0 )\cb1 \
\
\cb2 SHUFFLE = \cf5 True\cf0 \cb1 \
\
\cb2 VOCAB_SIZE\cb1  = 8130}