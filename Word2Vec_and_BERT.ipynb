{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBSeRGalA07XlV5U7IPhng",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/word2vec-and-BERT/blob/main/Word2Vec_and_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec and BERT"
      ],
      "metadata": {
        "id": "1KnojeRiYuPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook will be to motivate a more perfect understanding of BERT by showing how **CBOW Word2Vec** works and using it as a jumping-off point to motivate **BERT**. To those who know about these two methods, it's pretty clear how the two are conceptually related. I want to use this notebook to explore and explain the relationship between the two since BERT's training objective is similar to CBOW W2V's but contains some key differences.\n",
        "\n",
        "To this end, I will be following, commenting on, and reimplementing the relevant models from the papers in which they were introduced: [**Efficient Implementation of Word Representations in Vector Space**](https://arxiv.org/pdf/1301.3781.pdf) (**Word2Vec**) and [**BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805.pdf) (**BERT**)."
      ],
      "metadata": {
        "id": "GXNE_KvVZyVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tqdm\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchtext import get_tokenizer\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103 #our datasets for this project\n",
        "\n",
        "DATASET_small = \"WikiText2\"\n",
        "DATASET_large = \"WikiText103\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATASET = DATASET_small\n",
        "TOKENIZER=\"basic_english\"\n",
        "\n",
        "RUN_TRAINING_EXAMPLES = True"
      ],
      "metadata": {
        "id": "Mn7U7lxufR3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Dataset"
      ],
      "metadata": {
        "id": "7KKwvfN-Vu8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the **WikiText2** and **WikiText103** datasets available from `torchtext.datasets`. These are both compiled from wikipedia articles and have 2M and 100M+ tokens in them."
      ],
      "metadata": {
        "id": "JeJrJ7T1VzoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_dataset(dataset = DATASET, split=None):\n",
        "  if dataset == DATASET_small:\n",
        "    iter = WikiText2(split=split)\n",
        "  elif dataset == DATASET_large:\n",
        "    iter = WikiText103(split=split)\n",
        "  else:\n",
        "    raise ValueError(f\"{dataset} is not a valid dataset\")\n",
        "  return torchtext.data.to_map_style_dataset(iter)\n",
        "\n",
        "def build_vocab(data_iter, tokenizer=None):\n",
        "  if tokenizer is None:\n",
        "    tokenizer = get_tokenizer(TOKENIZER)\n",
        "    vocab = build_vocab_from_iterator(\n",
        "        map(tokenizer, data_iter),\n",
        "        specials=[\"<unk>\"],\n",
        "    )\n",
        "    vocab.set_default_index(vocab[\"<unk>\"])\n",
        "  return vocab\n"
      ],
      "metadata": {
        "id": "A-wJ2CPcUFcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec"
      ],
      "metadata": {
        "id": "RTHuERsBe8wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Motivation:\n",
        "The major motivation for Word2Vec when it was developed was that existing methods often failed to capture similarity when they developed representations of words in a vocabulary. Word2Vec aims to fix that.\n",
        "\n",
        "The methods used in Word2Vec were inspired by [**Linguistic Regularities in Continuous Word Representations**](https://aclanthology.org/N13-1090.pdf). A key feature was that similar words would have vectors that were close to one another and that words could have **many degrees of similarity** in their vector representations. Word2Vec is inspired by this approach and aims to make these vectors as accurate as possible."
      ],
      "metadata": {
        "id": "weBOiu61fAHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CBOW Architecture\n",
        "\n",
        "The architecture described in the paper uses a simple Feed-Forward Network with an embedding layer and no hidden layer. The input and output layers are both the size of the vocabulary given by `vocab_size` and the dimension of the embedding layer is given `embedding_dim`."
      ],
      "metadata": {
        "id": "k6R2UxrwKHjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim=300): #in the paper the embedding dimension suggested is 300\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.output(self.embedding(x))"
      ],
      "metadata": {
        "id": "yuVKeZTVd84Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preparation"
      ],
      "metadata": {
        "id": "wq-2IbUfbz_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to generate training examples for the CBOW model. CBOW is trained by passing in a certain number of words neighboring the target word in the vocab (without respect to order of the words) and then training the model to predict the target."
      ],
      "metadata": {
        "id": "DuYoWldoipY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#need to define some constants for data\n",
        "MAX_PARAGRAPH_LEN = 256\n",
        "CTX_WORDS = 2"
      ],
      "metadata": {
        "id": "Q6AL-7Riw8WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create function to collate data into inputs and outputs\n",
        "#batches will be paragraphs from the dataset\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def collate(batch, text_pipeline):\n",
        "  \"\"\"\n",
        "  Collate paragraphs into inputs and outputs for CBOW.\n",
        "  Inputs are word contexts and outputs are target words.\n",
        "    Context lengths are determined by CTX_WORDS\n",
        "  \"\"\"\n",
        "  batch_input, batch_output = [], []\n",
        "  for text in batch:\n",
        "    text = text_pipeline(text)\n",
        "    text = text[:MAX_PARAGRAPH_LEN] #truncate too-long paragraphs\n",
        "    if len(text)<(2*CTX_WORDS+1):\n",
        "      continue\n",
        "\n",
        "    for i in range(len(text)-(CTX_WORDS*2+1)):\n",
        "      window = text[i:i+CTX_WORDS*2+1]\n",
        "      inputs = window[:CTX_WORDS]+window[CTX_WORDS+1:]\n",
        "      outputs = window[CTX_WORDS]\n",
        "      batch_input.append(inputs)\n",
        "      batch_output.append(outputs)\n",
        "\n",
        "  batch_input = torch.Tensor(batch_input, dtype=torch.long)\n",
        "  batch_output = torch.Tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "  return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(\n",
        "    dataset, dataset_split, batch_size, shuffle, vocab=None\n",
        "):\n",
        "  data_iter = fetch_dataset(dataset, dataset_split)\n",
        "  tokenizer = get_tokenizer(TOKENIZER)\n",
        "  vocab = build_vocab(data_iter, tokenizer)\n",
        "\n",
        "  text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      data_iter = data_iter,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      collate_fn = partial(collate_fn, text_pipeline)\n",
        "  )\n",
        "\n",
        "  return dataloader, vocab"
      ],
      "metadata": {
        "id": "BDN1xrA6bzit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training"
      ],
      "metadata": {
        "id": "1CTbiF5Kievf"
      }
    }
  ]
}