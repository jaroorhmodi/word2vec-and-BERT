{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtfF2FPbibKNjeivhXdtML",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaroorhmodi/word2vec-and-BERT/blob/main/Word2Vec_and_BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word2Vec and BERT"
      ],
      "metadata": {
        "id": "1KnojeRiYuPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this notebook will be to motivate a more perfect understanding of BERT by showing how **CBOW Word2Vec** works and using it as a jumping-off point to motivate **BERT**. To those who know about these two methods, it's pretty clear how the two are conceptually related. I want to use this notebook to explore and explain the relationship between the two since BERT's training objective is similar to CBOW W2V's but contains some key differences.\n",
        "\n",
        "To this end, I will be following, commenting on, and reimplementing the relevant models from the papers in which they were introduced: [**Efficient Implementation of Word Representations in Vector Space**](https://arxiv.org/pdf/1301.3781.pdf) (**Word2Vec**) and [**BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding**](https://arxiv.org/pdf/1810.04805.pdf) (**BERT**)."
      ],
      "metadata": {
        "id": "GXNE_KvVZyVn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'portalocker>=2.0.0'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL17qDCBk9Tq",
        "outputId": "82e36399-2111-47a0-942d-045d7386aa9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker>=2.0.0 in /usr/local/lib/python3.10/dist-packages (2.8.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torchtext.data import to_map_style_dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.datasets import WikiText2, WikiText103 #our datasets for this project\n",
        "\n",
        "DATASET_small = \"WikiText2\"\n",
        "DATASET_large = \"WikiText103\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DATASET = DATASET_large\n",
        "TOKENIZER=\"basic_english\"\n",
        "DATA_DIRECTORY = \"/content/data\"\n",
        "\n",
        "RUN_TRAINING_EXAMPLES = True"
      ],
      "metadata": {
        "id": "Mn7U7lxufR3S"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Dataset"
      ],
      "metadata": {
        "id": "7KKwvfN-Vu8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be using the **WikiText2** and **WikiText103** datasets available from `torchtext.datasets`. These are both compiled from wikipedia articles and have 2M and 100M+ tokens in them."
      ],
      "metadata": {
        "id": "JeJrJ7T1VzoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MIN_WORD_FREQUENCY = 200 #min frequency to appear in vocab\n",
        "#HAD TO INCREASE FOR WT103 BECAUSE T4s ran out of memory on the very large onehot vectors"
      ],
      "metadata": {
        "id": "WkxaWdtUb-QH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_dataset(dataset = DATASET, split=None):\n",
        "  if dataset == DATASET_small:\n",
        "    iter = WikiText2(root=DATA_DIRECTORY, split=split)\n",
        "  elif dataset == DATASET_large:\n",
        "    iter = WikiText103(root=DATA_DIRECTORY, split=split)\n",
        "  else:\n",
        "    raise ValueError(f\"{dataset} is not a valid dataset\")\n",
        "  iter = to_map_style_dataset(iter)\n",
        "  return iter\n",
        "\n",
        "def build_vocab(data_iter, tokenizer=None):\n",
        "  if tokenizer is None:\n",
        "    tokenizer = get_tokenizer(TOKENIZER)\n",
        "  vocab = build_vocab_from_iterator(\n",
        "      map(tokenizer, data_iter),\n",
        "      specials=[\"<unk>\"],\n",
        "      min_freq=MIN_WORD_FREQUENCY\n",
        "  )\n",
        "  vocab.set_default_index(vocab[\"<unk>\"])\n",
        "  return vocab\n"
      ],
      "metadata": {
        "id": "A-wJ2CPcUFcL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word2Vec"
      ],
      "metadata": {
        "id": "RTHuERsBe8wW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Motivation:\n",
        "The major motivation for Word2Vec when it was developed was that existing methods often failed to capture similarity when they developed representations of words in a vocabulary. Word2Vec aims to fix that.\n",
        "\n",
        "The methods used in Word2Vec were inspired by [**Linguistic Regularities in Continuous Word Representations**](https://aclanthology.org/N13-1090.pdf). A key feature was that similar words would have vectors that were close to one another and that words could have **many degrees of similarity** in their vector representations. Word2Vec is inspired by this approach and aims to make these vectors as accurate as possible."
      ],
      "metadata": {
        "id": "weBOiu61fAHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###CBOW Architecture\n",
        "\n",
        "The architecture described in the paper uses a simple Feed-Forward Network with an embedding layer and no hidden layer. The input and output layers are both the size of the vocabulary given by `vocab_size` and the dimension of the embedding layer is given `embedding_dim`."
      ],
      "metadata": {
        "id": "k6R2UxrwKHjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, embedding_max_norm):\n",
        "    super(CBOW, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim, max_norm = embedding_max_norm)\n",
        "    self.output = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #embed, take mean, then predict\n",
        "    return self.output(self.embedding(x).mean(axis=1))"
      ],
      "metadata": {
        "id": "yuVKeZTVd84Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Data Preparation"
      ],
      "metadata": {
        "id": "wq-2IbUfbz_9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to generate training examples for the CBOW model. CBOW is trained by passing in a certain number of words neighboring the target word in the vocab (without respect to order of the words) and then training the model to predict the target."
      ],
      "metadata": {
        "id": "DuYoWldoipY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#need to define some constants for data\n",
        "MAX_PARAGRAPH_LEN = 256\n",
        "CTX_WORDS = 2"
      ],
      "metadata": {
        "id": "Q6AL-7Riw8WM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create function to collate data into inputs and outputs\n",
        "#batches will be paragraphs from the dataset\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def collate(batch, text_pipeline):\n",
        "  \"\"\"\n",
        "  Collate paragraphs into inputs and outputs for CBOW.\n",
        "  Inputs are word contexts and outputs are target words.\n",
        "    Context lengths are determined by CTX_WORDS\n",
        "  \"\"\"\n",
        "  batch_input, batch_output = [], []\n",
        "  for text in batch:\n",
        "    text = text_pipeline(text)\n",
        "    text = text[:MAX_PARAGRAPH_LEN] #truncate too-long paragraphs\n",
        "    if len(text)<(2*CTX_WORDS+1):\n",
        "      continue\n",
        "\n",
        "    for i in range(len(text)-(CTX_WORDS*2+1)):\n",
        "      window = text[i:i+CTX_WORDS*2+1]\n",
        "      inputs = window[:CTX_WORDS]+window[CTX_WORDS+1:]\n",
        "      outputs = window[CTX_WORDS]\n",
        "      batch_input.append(inputs)\n",
        "      batch_output.append(outputs)\n",
        "\n",
        "  batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
        "  batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
        "\n",
        "  return batch_input, batch_output\n",
        "\n",
        "\n",
        "def get_dataloader_and_vocab(\n",
        "    dataset, dataset_split, collate_fn, batch_size, shuffle, vocab=None\n",
        "):\n",
        "  data_iter = fetch_dataset(dataset, dataset_split)\n",
        "  tokenizer = get_tokenizer(TOKENIZER)\n",
        "  if vocab is None:\n",
        "    vocab = build_vocab(data_iter, tokenizer)\n",
        "\n",
        "  text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "\n",
        "  dataloader = DataLoader(\n",
        "      dataset=data_iter,\n",
        "      batch_size = batch_size,\n",
        "      shuffle = shuffle,\n",
        "      collate_fn = partial(collate_fn, text_pipeline=text_pipeline)\n",
        "  )\n",
        "\n",
        "  return dataloader, vocab"
      ],
      "metadata": {
        "id": "BDN1xrA6bzit"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model Training"
      ],
      "metadata": {
        "id": "1CTbiF5Kievf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Training utilities and constants\n",
        "\n",
        "Defining a trainer class for CBOW model and some useful constants for when we are setting up the run."
      ],
      "metadata": {
        "id": "RPkpmyelbSPN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOWTrainer:\n",
        "  def __init__(\n",
        "      self,\n",
        "      model,\n",
        "      train_dataloader,\n",
        "      val_dataloader,\n",
        "      loss_fn,\n",
        "      optimizer,\n",
        "      lr_scheduler,\n",
        "      num_epochs=1,\n",
        "      report_epochs=1):\n",
        "\n",
        "    self.model = model\n",
        "    self.train_data = train_dataloader\n",
        "    self.val_data = val_dataloader\n",
        "    self.loss_fn = loss_fn\n",
        "    self.optimizer = optimizer\n",
        "    self.lr_scheduler = lr_scheduler\n",
        "    self.train_loss = []\n",
        "    self.val_loss = []\n",
        "    self.num_epochs = num_epochs\n",
        "    self.report_epochs = report_epochs\n",
        "\n",
        "  def train(self):\n",
        "    for epoch in range(self.num_epochs):\n",
        "      # print(f\"Training epoch:{epoch}\")\n",
        "      self.train_epoch()\n",
        "      self.val_epoch()\n",
        "\n",
        "      if epoch%self.report_epochs==0 or epoch==self.num_epochs-1:\n",
        "        print(f\"=====EPOCH:{epoch+1}/{self.num_epochs}=====\")\n",
        "        print(f\"Train Loss: {self.train_loss[-1]:.5f}\")\n",
        "        print(f\"Valid Loss: {self.val_loss[-1]:.5f}\\n\")\n",
        "\n",
        "      self.lr_scheduler.step()\n",
        "\n",
        "  def train_epoch(self):\n",
        "    self.model.train()\n",
        "    running_loss = []\n",
        "    for iter, batch in enumerate(self.train_data):\n",
        "      X, y = batch\n",
        "      X = X.to(DEVICE)\n",
        "      y = y.to(DEVICE)\n",
        "\n",
        "      #zero out gradient on optimizer\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      #forward pass\n",
        "      prediction = self.model(X)\n",
        "\n",
        "      #get loss\n",
        "      batch_loss = self.loss_fn(prediction, y)\n",
        "\n",
        "      #backpropagate\n",
        "      batch_loss.backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "      running_loss.append(batch_loss.item())\n",
        "\n",
        "    epoch_loss = np.mean(running_loss)\n",
        "    self.train_loss.append(epoch_loss)\n",
        "\n",
        "  def val_epoch(self):\n",
        "    self.model.eval()\n",
        "    running_loss = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for iter, batch in enumerate(self.val_data):\n",
        "        X, y = batch\n",
        "        X = X.to(DEVICE)\n",
        "        y = y.to(DEVICE)\n",
        "\n",
        "        #inference\n",
        "        prediction = self.model(X)\n",
        "\n",
        "        #loss\n",
        "        batch_loss = self.loss_fn(prediction, y)\n",
        "\n",
        "        running_loss.append(batch_loss.item())\n",
        "\n",
        "    epoch_loss = np.mean(running_loss)\n",
        "    self.val_loss.append(epoch_loss)\n",
        "\n",
        "  def print_losses(self):\n",
        "    for epoch, losses in enumerate(zip(self.train_loss, self.val_loss)):\n",
        "      tloss, vloss = losses\n",
        "      print(f\"=====EPOCH:{epoch+1}/{self.num_epochs}=====\")\n",
        "      print(f\"Train Loss: {tloss}\")\n",
        "      print(f\"Valid loss: {vloss}\\n\")"
      ],
      "metadata": {
        "id": "nuHGzVKDMJg1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Training a CBOW model"
      ],
      "metadata": {
        "id": "FYfDq4y2bisa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#flag to not train a new model if existing one is available:\n",
        "TRAIN=True"
      ],
      "metadata": {
        "id": "9gwzh6GZbHMW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMBEDDING_DIM = 300 #in the paper the recommended dimension is 300\n",
        "EMBEDDING_MAX_NORM = 1 #make sure embedding vectors are no longer than unit vectors\n",
        "\n",
        "NUM_EPOCHS = 50 #number of epochs in training\n",
        "REPORT_EPOCHS = 5 #print losses every REPORT_EPOCHS epochs\n",
        "TRAIN_BATCH_SIZE = 128\n",
        "VAL_BATCH_SIZE = 128\n",
        "\n",
        "LR = 1\n",
        "\n",
        "torch.manual_seed(2118)\n",
        "\n",
        "SHUFFLE = True"
      ],
      "metadata": {
        "id": "5iegJAEQiq9g"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Data\n",
        "train_dataloader, vocab = get_dataloader_and_vocab(DATASET, \"train\", collate, TRAIN_BATCH_SIZE, SHUFFLE)\n",
        "val_dataloader, _ = get_dataloader_and_vocab(DATASET, \"valid\", collate, VAL_BATCH_SIZE, SHUFFLE, vocab)\n",
        "\n",
        "VOCAB_SIZE = len(vocab.get_stoi())\n",
        "VOCAB_SIZE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLVBIi8WbPTx",
        "outputId": "6d9d27c7-7a40-44e3-8213-f9056f52f2fc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21889"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make our model\n",
        "cbow_model = CBOW(VOCAB_SIZE, EMBEDDING_DIM, EMBEDDING_MAX_NORM).to(DEVICE)"
      ],
      "metadata": {
        "id": "ah8_EPpBii4Y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make loss function, optimizer, and lr_scheduler\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(cbow_model.parameters(), lr=LR)\n",
        "lr_scheduler = LambdaLR(\n",
        "    optimizer,\n",
        "    lr_lambda= lambda epoch: 1-epoch/NUM_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "4rCPqVgIqQHj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make trainer\n",
        "trainer = CBOWTrainer(\n",
        "    cbow_model,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    lr_scheduler,\n",
        "    NUM_EPOCHS,\n",
        "    REPORT_EPOCHS\n",
        ")"
      ],
      "metadata": {
        "id": "J2gVnc6WkkdE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "  trainer.train()\n",
        "  CBOW_MODEL = trainer.model"
      ],
      "metadata": {
        "id": "LIhELJ6RpTCf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CBOW_VERSION = 30 #V3 was the best model on WikiText2, V30 will be WikiText103 model\n",
        "PATH = '/content/models/'\n",
        "MODEL_PATH = os.path.join(PATH, f'CBOW_{DATASET}_v{CBOW_VERSION}.pth')\n",
        "VOCAB_PATH = os.path.join(PATH, f'vocab/vocab_CBOW_{DATASET}_v{CBOW_VERSION}.pth')\n",
        "\n",
        "if TRAIN:\n",
        "  torch.save(trainer.model.state_dict(), f'/content/models/CBOW_{DATASET}_v{CBOW_VERSION}.pth')\n",
        "  torch.save(vocab, VOCAB_PATH)\n",
        "  trainer.print_losses"
      ],
      "metadata": {
        "id": "gEzwaqbY9JIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if TRAIN:\n",
        "  CBOW_MODEL = trainer.model.state_dict()\n",
        "  CBOW_VOCAB = vocab\n",
        "else:\n",
        "  CBOW_MODEL =\n",
        "  CBOW_MODEL = torch.load(MODEL_PATH, map_location = DEVICE)\n",
        "  CBOW_VOCAB = torch.load(VOCAB_PATH)"
      ],
      "metadata": {
        "id": "yVV3jHpWcByf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Inference with CBOW"
      ],
      "metadata": {
        "id": "heGeivwWZTuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With CBOW our goal is to use the embedding parameters we learned as part of training the model to embed words as vectors."
      ],
      "metadata": {
        "id": "F02h0P1oZjZw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Getting word embeddings"
      ],
      "metadata": {
        "id": "NjSkuCLMcxSe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CBOW_MODEL['embedding.weight']"
      ],
      "metadata": {
        "id": "Sl1oJdRyf-kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(trainer.model.parameters())[0]"
      ],
      "metadata": {
        "id": "JfuwPXdDgcuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.state_dict()['embedding.weight']"
      ],
      "metadata": {
        "id": "5w_CRMrmgSRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#EMBEDDINGS ARE IN THE FIRST LAYER OF THE MODEL\n",
        "embeddings = CBOW_MODEL['embedding.weight']\n",
        "embeddings = embeddings.cpu().detach().numpy()\n",
        "\n",
        "#NORMALIZE EMBEDDINGS BASED ON NORMS\n",
        "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
        "norms = np.reshape(norms, (len(norms), 1))\n",
        "embeddings_norm = embeddings / norms\n",
        "embeddings_norm.shape"
      ],
      "metadata": {
        "id": "1PWQhLrlHUyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Testing word vectors"
      ],
      "metadata": {
        "id": "fregpXdDdPZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_N_best_matches(\n",
        "    word,\n",
        "    N,\n",
        "    embeddings_norm = embeddings_norm,\n",
        "    vocab = CBOW_VOCAB\n",
        "):\n",
        "  word_idx = vocab[word]\n",
        "  if word_idx == 0:\n",
        "    print(\"OOV word\")\n",
        "    return None\n",
        "  word_vec = embeddings_norm[word_idx]\n",
        "  word_vec = np.reshape(word_vec, (len(word_vec), 1))\n",
        "  dists = np.matmul(embeddings_norm, word_vec).flatten() #dot prods\n",
        "  topN_ids = np.argsort(-dists)[1 : N + 1]\n",
        "  topN_dict = {}\n",
        "  for sim_word_id in topN_ids:\n",
        "      sim_word = vocab.lookup_token(sim_word_id)\n",
        "      topN_dict[sim_word] = dists[sim_word_id]\n",
        "  return topN_dict"
      ],
      "metadata": {
        "id": "drNqhhbGdR_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, sim in get_N_best_matches(\"brother\", 50).items():\n",
        "    print(\"{}: {:.3f}\".format(word, sim))"
      ],
      "metadata": {
        "id": "N58iLItPer1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb1 = embeddings[vocab[\"father\"]]\n",
        "emb2 = embeddings[vocab[\"man\"]]\n",
        "emb3 = embeddings[vocab[\"woman\"]]\n",
        "\n",
        "emb4 = emb1 - emb2 + emb3\n",
        "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
        "emb4 = emb4 / emb4_norm\n",
        "\n",
        "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
        "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
        "\n",
        "top5 = np.argsort(-dists)[:10]\n",
        "\n",
        "for word_id in top5:\n",
        "    print(\"{}: {:.3f}\".format(vocab.lookup_token(word_id), dists[word_id]))"
      ],
      "metadata": {
        "id": "6YCl15qZhLSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0leoVQOxhol_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}